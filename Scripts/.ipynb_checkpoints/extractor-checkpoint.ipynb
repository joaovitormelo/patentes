{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS E FUNÇÕES GERAIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.etree as tree\n",
    "from unidecode import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import os\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parte constante em todos os caminhos do codigo (adaptar à organização de pasta)\n",
    "#cosnt_path deve ser o caminho para a pasta principal (pode ser relativo, a partir da pasta onde esta\n",
    "#o arquivo de codigo), onde deverao haver as pastas: Nodes, Edges e Arquivos\n",
    "\n",
    "const_path = '..'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "def words(text):\n",
    "    return text.split(' ')\n",
    "\n",
    "def normalize(text, key_word=False):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    text = unidecode(text)\n",
    "    stop_words = [',', '.', '!', '-', ':', '?', '/', '\\\\', '|', ';']\n",
    "    if key_word:\n",
    "        text = text.lower()\n",
    "        stop_words += stopwords.words('portuguese') + stopwords.words('english')\n",
    "        text = [stemmer.stem(word) for word in words(text) if word not in stop_words]\n",
    "    else:\n",
    "        text = text.upper()\n",
    "        text = text.replace('_', ' ')\n",
    "        text = text.replace(';', '')\n",
    "        text = [word for word in words(text) if word not in stop_words]\n",
    "    \n",
    "    text = ' '.join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRAÇÃO DE ARESTAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_network(network, xml, mainId, year):\n",
    "    numPatents = 0\n",
    "    patents = xml.xpath('//PATENTE')\n",
    "    for patente in patents:\n",
    "        if (int(patente.xpath('./DADOS-BASICOS-DA-PATENTE/@ANO-DESENVOLVIMENTO')[0]) != year):\n",
    "            continue\n",
    "        numPatents += 1\n",
    "        key = normalize(patente.xpath('.//DADOS-BASICOS-DA-PATENTE/@TITULO')[0], True)\n",
    "        try:\n",
    "            network[key]\n",
    "        except:\n",
    "            network[key] = [mainId]\n",
    "        else:\n",
    "            if mainId not in network[key]:\n",
    "                network[key].append(mainId)\n",
    "                    \n",
    "        authorsId = [author for author in patente.xpath('.//AUTORES/@NRO-ID-CNPQ') if ((author not in network[key]) and (len(author) > 0))]\n",
    "        authorsId = list(set(authorsId))\n",
    "        network[key] += authorsId\n",
    "    return numPatents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_edges(net_dict):\n",
    "    edges = []\n",
    "    for key in list(net_dict.keys()): \n",
    "        lista = list(itertools.combinations(net_dict[key], 2))\n",
    "        edges += lista\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(network, pasta, yearRange=[1980, 2019]):\n",
    "    folders = []\n",
    "    for root, dirs, files in os.walk(os.sep.join([const_path, 'Arquivos'])):\n",
    "        folders = dirs[1:101]\n",
    "        break\n",
    "    timeMean = 0\n",
    "    qPatents = 0\n",
    "    for i in range(len(folders)):\n",
    "        time = current_time()\n",
    "        print(folders[i], end=\"\\t\\t\")\n",
    "        subFolders = []\n",
    "        for root, dirs, files in os.walk(os.sep.join([const_path, 'Arquivos', folders[i]])):\n",
    "            subFolders = dirs\n",
    "            break\n",
    "        for subFolder in subFolders:\n",
    "            path = os.sep.join([const_path, 'Arquivos', folders[i], subFolder, 'curriculo.xml'])\n",
    "            xml = tree.parse(path)\n",
    "            qPatents += extract_network(network, xml, folders[i] + subFolder, yearRange[0])  \n",
    "        timeMean += (current_time() - time)\n",
    "        mean = timeMean / (i+1)\n",
    "        timeLeft = int(mean*(99-i-1)/1000)\n",
    "        minutes = int(timeLeft/60)\n",
    "        seconds = timeLeft%60\n",
    "        print('Tempo previsto: %02d:%02dm' % (minutes, seconds))\n",
    "    print(\"Etapa Concluida\")\n",
    "    return qPatents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRAI ARESTAS. CHAMA LOAD_DOCUMENTS POR ANO, SALVANDO AS ARESTAS EM ARQUIVOS DE CADA ANO E TAMBÉM\n",
    "#ACUMULANDO EM UM ARQUIVO DAS ARESTAS DE PERIODOS DE 5 ANOS\n",
    "\n",
    "def process():\n",
    "    lenEdges = 0\n",
    "    year = 1980\n",
    "    yearF = 2019\n",
    "    print('EXTRAÇÃO DE ARESTAS')\n",
    "    finalPeriodo = year + 4\n",
    "    fileNamePeriodo = \"periodo-\" + str(year) + \"-\" + str(finalPeriodo) + \".csv\"\n",
    "    print('\\tANOS ' + str(year) + ' A ' + str(finalPeriodo))\n",
    "    \n",
    "    #cria arquivo vazio do primeiro periodo temporal (80-84) - passa parametro edges como um dicionario vazio, apenas para gerar o arquivo\n",
    "    write_edges(dict(), fileNamePeriodo, 'w')\n",
    "    \n",
    "    for i in range(year, yearF+1):\n",
    "        print('ANO ' + str(i))\n",
    "        network = dict()\n",
    "        qPatents = 0\n",
    "        \n",
    "        try:\n",
    "            qPatents = load_documents(network, [i, finalPeriodo])\n",
    "        except:\n",
    "            print('Operacao cancelada!')\n",
    "            print(network)\n",
    "            del network\n",
    "            break\n",
    "            \n",
    "        edges = make_edges(network)\n",
    "        lenEdges += len(edges)\n",
    "        write_edges(edges, 'ano-' + str(i) + '.csv', 'w')\n",
    "        write_edges(edges, fileNamePeriodo, 'a')\n",
    "        if i == finalPeriodo:\n",
    "            finalPeriodo += 5\n",
    "            fileNamePeriodo = str(i) + \"-\" + str(finalPeriodo) + \".csv\"\n",
    "            write_edges(dict(), fileNamePeriodo, 'w')\n",
    "            print('\\tANOS ' + str(i) + ' A ' + str(finalPeriodo))\n",
    "            \n",
    "        if i == year:\n",
    "            write_quant((i, qPatents), 'w')\n",
    "        else:\n",
    "            write_quant((i, qPatents), 'a')\n",
    "            \n",
    "        del edges\n",
    "        del network\n",
    "\n",
    "    return lenEdges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_edges(edges, name, m):\n",
    "    with open(os.sep.join([const_path, 'Edges', name]), m) as f:\n",
    "        if m == 'w':\n",
    "            f.write('Source;Target\\n')\n",
    "        for edge in edges:\n",
    "            f.write(edge[0] + '*;' + edge[1] + '*\\n')\n",
    "            \n",
    "def write_quant(data, m):\n",
    "    with open(os.sep.join([const_path, 'Edges', 'years.csv']), m) as f:\n",
    "        if m == 'w':\n",
    "            f.write('Ano;Quantidade de patentes\\n')\n",
    "        f.write(str(data[0]) + ';' + str(data[1]) + '\\n')\n",
    "    \n",
    "def info(infoC):\n",
    "    print('Arestas extraídas: ' + str(infoC))\n",
    "    \n",
    "def edges():\n",
    "    info(process())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EXTRAÇÃO DE NÓS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkStatus(titulos, formation):\n",
    "    xpathStr = './/' + formation + '[@STATUS-DO-CURSO=\"CONCLUIDO\"]'\n",
    "    return len(titulos.xpath(xpathStr))\n",
    "\n",
    "def greaterTitle(titulos):\n",
    "    \n",
    "    greater = ''\n",
    "    \n",
    "    if checkStatus(titulos, 'POS-DOUTORADO'):\n",
    "        greater = 'POS-DOUTORADO'\n",
    "    elif checkStatus(titulos, 'DOUTORADO'):\n",
    "        greater = 'DOUTORADO'\n",
    "    elif checkStatus(titulos, 'POS-MESTRADO'):\n",
    "        greater = 'POS-MESTRADO'\n",
    "    elif checkStatus(titulos, 'MESTRADO'):\n",
    "        greater = 'MESTRADO'\n",
    "    elif checkStatus(titulos, 'POS-GRADUACAO'):\n",
    "        greater = 'POS-GRADUACAO'\n",
    "    elif checkStatus(titulos, 'GRADUACAO'):\n",
    "        greater = 'GRADUACAO'\n",
    "        \n",
    "    return greater\n",
    "\n",
    "def getValue(xml, xmlStr):\n",
    "    try:\n",
    "        value = normalize(xml.xpath(xmlStr)[0])\n",
    "    except:\n",
    "        value = \"\"\n",
    "    return value\n",
    "\n",
    "def empty_treatment(nodes):\n",
    "    types = [\"DESCONHECIDO\",0,\"DESCONHECIDA\",\"DESCONHECIDA\",\"DESCONHECIDA\",\"DESCONHECIDA\",\"DESCONHECIDO\",\"DESCONHECIDO\",\"DESCONHECIDA\",0,0,0,0,0,0,0,0]\n",
    "    for patent in nodes.values():\n",
    "        for i in range(len(patent)):\n",
    "            if patent[i] == \"\" or patent[i] == None:\n",
    "                patent[i] = types[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node_detail(xml):\n",
    "    bigArea = ''\n",
    "    area = ''\n",
    "    firstArea = ''\n",
    "    index = 0\n",
    "    areas = xml.xpath('//AREAS-DE-ATUACAO/AREA-DE-ATUACAO')\n",
    "    for i in range(len(areas)):\n",
    "        if areas[i].xpath('./@NOME-GRANDE-AREA-DO-CONHECIMENTO')[0] == \"ENGENHARIAS\":\n",
    "            index = i\n",
    "            break\n",
    "    try:\n",
    "        bigArea = normalize(areas[index].xpath('./@NOME-GRANDE-AREA-DO-CONHECIMENTO')[0])\n",
    "        area = normalize(areas[index].xpath('./@NOME-DA-AREA-DO-CONHECIMENTO')[0])\n",
    "    except:\n",
    "        pass\n",
    "    institution = getValue(xml, '//ENDERECO-PROFISSIONAL/@NOME-INSTITUICAO-EMPRESA')\n",
    "    country = getValue(xml, '//DADOS-GERAIS/@SIGLA-PAIS-NACIONALIDADE')\n",
    "    if country == None or country == \"\":\n",
    "        country = getValue(xml, '//DADOS-GERAIS/@PAIS-DE-NASCIMENTO')\n",
    "        try:\n",
    "            country.index('ESTADOS UNIDOS')\n",
    "        except:\n",
    "            country = country[0:3]\n",
    "        else:\n",
    "            country = \"EUA\"\n",
    "    state = getValue(xml, '//DADOS-GERAIS/@UF-NASCIMENTO')\n",
    "    city = getValue(xml, '//DADOS-GERAIS/@CIDADE-NASCIMENTO')\n",
    "    qArticles = len(xml.xpath('//ARTIGO-PUBLICADO'))\n",
    "    qWorksA = len(xml.xpath('//TRABALHO-EM-EVENTOS/DADOS-BASICOS-DO-TRABALHO[@NATUREZA=\"COMPLETO\"]'))\n",
    "    qWorksT = len(xml.xpath('//TRABALHO-TECNICO'))\n",
    "    qOrientations = len(xml.xpath('//ORIENTACOES-CONCLUIDAS-PARA-MESTRADO')) + len(xml.xpath('//ORIENTACOES-CONCLUIDAS-PARA-DOUTORADO')) + len(xml.xpath('//OUTRAS-ORIENTACOES-CONCLUIDAS'))\n",
    "    qThesisD = len(xml.xpath('//DOUTORADO'))\n",
    "    qThesisM = len(xml.xpath('//MESTRADO'))\n",
    "    qTCC = len(xml.xpath('//GRADUACAO'))\n",
    "    qIC = len(xml.xpath('//DADOS-BASICOS-DE-OUTRAS-ORIENTACOES-CONCLUIDAS[@NATUREZA=\"INICIACAO_CIENTIFICA\"]'))\n",
    "    return [bigArea, area, institution, country, state, city, qArticles, qWorksA, qWorksT, qOrientations, qThesisD, qThesisM, qTCC, qIC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_node(nodes, xml, mainId, yearRange):\n",
    "    #informaçoes de cada curriculo\n",
    "    name = getValue(xml, '//DADOS-GERAIS/@NOME-COMPLETO')\n",
    "    try:\n",
    "        titles = xml.xpath('//FORMACAO-ACADEMICA-TITULACAO')[0]\n",
    "        title = greaterTitle(titles)\n",
    "    except:\n",
    "        if (name == ''): #se der erro ao tentar acessar os titulos e name esta vazio, é por que o XML está vazio (erro ao carregar currículo)\n",
    "            return mainId\n",
    "    patents = xml.xpath('//PATENTE')\n",
    "    \n",
    "    inRange = False\n",
    "    for patent in patents:\n",
    "        try:\n",
    "            if (int(patent.xpath('./DADOS-BASICOS-DA-PATENTE/@ANO-DESENVOLVIMENTO')[0]) in range(yearRange[0], yearRange[1])):\n",
    "                inRange = True\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "    if not inRange:\n",
    "        return None\n",
    "    \n",
    "    nodes[mainId] = [name, len(patents), title] + extract_node_detail(xml)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nodes(nodes, yearRange=[1980, 2019]):\n",
    "    for i in range(yearRange[0], yearRange[1]+1):\n",
    "        nodes[str(i)] = dict()\n",
    "    for i in range(yearRange[0], yearRange[1]+1, 5):\n",
    "        f = i + 4\n",
    "        if f > yearRange[1]:\n",
    "            f = yearRange[1]\n",
    "        key = str(i) + \"-\" + str(f)\n",
    "        nodes[key] = dict()\n",
    "    folders = []\n",
    "    for root, dirs, files in os.walk(os.sep.join([const_path, 'Arquivos'])):\n",
    "        folders = dirs[1:101]\n",
    "        break\n",
    "    timeMean = 0\n",
    "    for i in range(len(folders)):\n",
    "        time = current_time()\n",
    "        print(folders[i], end=\"\\t\\t\")\n",
    "        subFolders = []\n",
    "        for root, dirs, files in os.walk(os.sep.join([const_path, 'Arquivos', folders[i]])):\n",
    "            subFolders = dirs\n",
    "            break\n",
    "        for subFolder in subFolders:\n",
    "            path = os.sep.join([const_path, 'Arquivos', folders[i], subFolder, 'curriculo.xml'])\n",
    "            xml = tree.parse(path)\n",
    "            for key in nodes.keys():\n",
    "                sYear = 0\n",
    "                eYear = 0\n",
    "                if (len(key) > 4):\n",
    "                    sYear = int(key[:4])\n",
    "                    eYear = int(key[5:]) + 1\n",
    "                else:\n",
    "                    sYear = int(key)\n",
    "                    eYear = sYear + 1\n",
    "                r = extract_node(nodes[key], xml, folders[i] + subFolder, (sYear, eYear))\n",
    "        timeMean += (current_time() - time)\n",
    "        mean = timeMean / (i+1)\n",
    "        timeLeft = int(mean*(99-i-1)/1000)\n",
    "        minutes = int(timeLeft/60)\n",
    "        seconds = timeLeft%60\n",
    "        print('Tempo previsto: %02d:%02dm' % (minutes, seconds))\n",
    "    print(\"Etapa Concluida\")\n",
    "    return len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_nodes(nodes, file):\n",
    "    with open(os.sep.join([const_path, 'Nodes', file]), 'w') as f:\n",
    "        f.write('Id;Nome;Num.Patentes;Formacao;GrandeArea;Area;Instituicao;Pais;Estado;Cidade;Q.ArtigosPublicados;Q.TrabalhosEmCongressos;Q.TrabalhosTecnicos;Q.Orientacoes;Q.TesesDoutorado;Q.DissertacaoMestrado;Q.TCC;Q.IC\\n')\n",
    "        for id_, node in nodes.items():\n",
    "            f.write(str(id_) + '*')\n",
    "            for attr in node:\n",
    "                f.write(';' + str(attr))\n",
    "            f.write('\\n')\n",
    "            \n",
    "def write_all_nodes(nodes):\n",
    "    for key in nodes.keys():\n",
    "        txt = ''\n",
    "        if (len(key) == 4):\n",
    "            txt = 'ano-'\n",
    "        write_nodes(nodes[key], txt+key+\".csv\")\n",
    "\n",
    "def process_nodes():\n",
    "    print('\\tEXTRAÇÃO DE NÓS')\n",
    "    nodes = dict()\n",
    "    try:\n",
    "        load_nodes(nodes)\n",
    "    except:\n",
    "        print('Operação cancelada!')\n",
    "        return\n",
    "    for yearNodes in nodes.values():\n",
    "        empty_treatment(yearNodes)\n",
    "    print(nodes)\n",
    "    write_all_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PESQUISA, NOS CURRÍCULOS, NÓS QUE FORAM APENAS REFERENCIADOS EM UMA COLABORAÇÃO DO ANO/PERÍODO POR UM \n",
    "#CURRÍCULO QUE NÃO SEJA O SEU, E NÃO POSSUEM UMA PATENTE NESSE ANO/PERÍODO REGISTRADA EM SEU CURRÍCULO\n",
    "\n",
    "def find_lost_nodes():\n",
    "    fileNames = []\n",
    "    #1980\n",
    "    for year in range(1980, 2020):\n",
    "        fileNames.append('ano-' + str(year))\n",
    "        if year%5==0:\n",
    "            fileNames.append(str(year) + '-' + str(year+4))\n",
    "    lostNodes = dict()\n",
    "    for name in fileNames:\n",
    "        with open(os.sep.join([const_path, 'Edges', name + '.csv']), 'r') as f:\n",
    "            colaborations = f.read().split('\\n')\n",
    "        ids = [two_ids.split(';')[i] for two_ids in colaborations[1:-1] for i in range(2)]\n",
    "        ids = list(set(ids))\n",
    "        \n",
    "        for id_ in ids:\n",
    "            if id_ not in lostNodes.keys():\n",
    "                lostNodes[id_] = []\n",
    "        with open(os.sep.join([const_path, 'Nodes', name + '.csv']), 'r') as f:\n",
    "            nodes = f.read().split('\\n')[1:-1]\n",
    "        for i in range(len(nodes)):\n",
    "            nodes[i] = nodes[i].split(';')[0]\n",
    "        for id_ in ids:\n",
    "            if id_ not in nodes:\n",
    "                lostNodes[id_] += [name]\n",
    "    return lostNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_nodes(lostNodes):\n",
    "    for root, dirs, files in os.walk(os.sep.join([const_path, 'Arquivos'])):\n",
    "        folders = dirs[1:101]\n",
    "        break\n",
    "    timeMean = 0\n",
    "    for i in range(len(folders)):\n",
    "        time = current_time()\n",
    "        print(folders[i], end=\"\\t\\t\")\n",
    "        subFolders = []\n",
    "        for root, dirs, files in os.walk(os.sep.join([const_path, 'Arquivos', folders[i]])):\n",
    "            subFolders = dirs\n",
    "            break\n",
    "        for subFolder in subFolders:\n",
    "            path = os.sep.join([folders[i], subFolder, 'curriculo.xml'])\n",
    "            xml = tree.parse(path)\n",
    "            parse_node(lostNodes, xml, folders[i] + subFolder)\n",
    "        timeMean += (current_time() - time)\n",
    "        mean = timeMean / (i+1)\n",
    "        timeLeft = int(mean*(99-i-1)/1000)\n",
    "        minutes = int(timeLeft/60)\n",
    "        seconds = timeLeft%60\n",
    "        print('Tempo previsto: %02d:%02dm' % (minutes, seconds))\n",
    "    print(\"Etapa Concluida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_node(id_, node, name):\n",
    "    with open(os.sep.join([const_path, 'Nodes', name]), 'a') as f:\n",
    "        f.write(str(id_) + '*')\n",
    "        for attr in node:\n",
    "            f.write(';' + str(attr))\n",
    "        f.write('\\n')\n",
    "\n",
    "def parse_node(lostNodes, xml, mainId):\n",
    "    if (mainId+'*') not in lostNodes.keys():\n",
    "        return\n",
    "    if len(lostNodes[mainId+'*']) == 0:\n",
    "        return\n",
    "    #informaçoes de cada curriculo\n",
    "    name = getValue(xml, '//DADOS-GERAIS/@NOME-COMPLETO')\n",
    "    try:\n",
    "        titles = xml.xpath('//FORMACAO-ACADEMICA-TITULACAO')[0]\n",
    "        title = greaterTitle(titles)\n",
    "    except:\n",
    "        if (name == ''): #se der erro ao tentar acessar os titulos e name esta vazio, é por que o XML está vazio (erro ao carregar currículo)\n",
    "            return mainId\n",
    "    patents = xml.xpath('//PATENTE')\n",
    "    \n",
    "    nodes = dict()\n",
    "    nodes[mainId] = [name, len(patents), title] + extract_node_detail(xml)\n",
    "    emptyTreatment(nodes)\n",
    "    for fileName in lostNodes[mainId+'*']:\n",
    "        add_node(mainId, nodes[mainId], fileName+'.csv')\n",
    "    lostNodes[mainId+'*'].append('RECUPERADO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nodes_search():\n",
    "    lostNodes = find_lost_nodes()\n",
    "    try:\n",
    "        search_nodes(lostNodes)\n",
    "    except:\n",
    "        print('Interrupção inesperada!\\n\\n')\n",
    "    return lostNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_nodes():\n",
    "    lostNodes = process_nodes_search()\n",
    "    recuperados = []\n",
    "    for id_, node in lostNodes.items():\n",
    "        if 'RECUPERADO' in node:\n",
    "            recuperados.append(id_)\n",
    "    print(len(recuperados))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mostra Nós que estavam faltando (chave) e lista de períodos/anos em que estavam faltando. Se foi recuperado, aparece\n",
    "#a String RECUPERADO na última posição da lista.\n",
    "def status_lost_nodes():\n",
    "    for key, files in lostNodes.items():\n",
    "        print(key + ' = ' + str(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes():\n",
    "    process_nodes()\n",
    "    print('Recuperando Nós')\n",
    "    recover_nodes()\n",
    "    status_lost_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global const_path\n",
    "    const_path = input('Insira o caminho constante para a pasta principal (pode ser relativo da pasta onde está o código ou absoluto)')\n",
    "    while True:\n",
    "        sair = False\n",
    "        try:\n",
    "            print('DIGITE OUTRA COISA PARA SAIR\\n1 - Extrair Arestas (Demorado)\\n2 - Extrair Nós\\n')\n",
    "            i = input()\n",
    "            if i == '1':\n",
    "                edges()\n",
    "            elif i == '2':\n",
    "                nodes()\n",
    "            else:\n",
    "                sair = True\n",
    "        except:\n",
    "            break\n",
    "        if sair:\n",
    "            break\n",
    "    print('Programa encerrando')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIGITE OUTRA COISA PARA SAIR\n",
      "1 - Extrair Arestas (Demorado)\n",
      "2 - Extrair Nós\n",
      "\n",
      "0\n",
      "Programa encerrando\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
